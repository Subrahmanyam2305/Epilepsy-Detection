{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],1) \n",
    "        #self.weights2   = np.random.rand(12,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "    \n",
    "    def feedforward(self):\n",
    "        self.layer1 = np.greater(np.zeros(self.input.shape[0]),np.dot(self.input, self.weights1))\n",
    "        #self.output = np.greater(np.zeros(self.input.shape[1]),np.dot(self.layer1, self.weights2))\n",
    "    \n",
    "    def relu_derivative(self,x):\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "\n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        #self.output_new = self.output[1:0] \n",
    "        #print(self.output_new.shape)\n",
    "        #self.output_new = np.reshape(self.output_new,)\n",
    "        #d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * relu_derivative(self.output)))\n",
    "        d_weights1 = (np.dot(2*(self.y - self.layer1) * self.relu_derivative(self.layer1), self.weights1.T))\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        #self.weights2 += d_weights2\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "X = genfromtxt('./sub_data/set1x.csv',delimiter=',')  # train data\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# output array\n",
    "Y = genfromtxt('./sub_data/sety1.csv',delimiter=',') # train data\n",
    "\n",
    "Y = np.reshape(Y,(7343,1))\n",
    "print(Y)\n",
    "\n",
    "\n",
    "a1 = NeuralNetwork(X,Y.T)\n",
    "a1.feedforward();\n",
    "a1.backprop();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99\n",
      "Train accuracy: 0.521585183168\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD79JREFUeJzt3F+MXGd5x/Hvj7UDSaAF6i2htpf4wqJKEIVq5FKDqhSa\n1oQIc5GLRaLiAsly1KihokKmSJFobytEi1JSC1JQobEQUNeKEtLwRyKV2uC1MSZ24rI1f2w3NCYI\nG1cRxvD0Yk/S0bLOnl3PepN5vx9p5HPe9z1nnke7/u3ZMzObqkKS1I4XrHYBkqTLy+CXpMYY/JLU\nGINfkhpj8EtSYwx+SWqMwS9JjekV/Em2JTmWZDbJrgXmb0hyJsmh7nHH0Nx3k3yrG58ZZfGSpKVb\ns9iCJBPAncCNwElgf5J9VXV03tKHqurmi5zm96vqh5dWqiRpFBYNfmALMFtVxwGS7AG2A/ODf2TW\nrVtX11577UqdXpLGzoEDB35YVZN91vYJ/vXAiaH9k8DvLLBua5LDwCngz6vqSDdewJeS/Bz4+6ra\nvdCTJNkB7ACYmppiZsa7QpLUV5Lv9V3bJ/j7OAhMVdW5JDcBe4HN3dybqupUkl8HHkzyWFV9bf4J\nuh8IuwEGg4F/QEiSVkifF3dPARuH9jd0Y8+oqrNVda7bvg9Ym2Rdt3+q+/cJ4J+Zu3UkSVolfYJ/\nP7A5yaYkVwDTwL7hBUmuSZJue0t33ieTXJ3kJd341cAfAo+MsgFJ0tIsequnqi4kuQ14AJgA7q6q\nI0l2dvN3AbcAtya5ADwFTFdVJXkF8M/dz4Q1wD9V1RdXqBdJUg95Lv49/sFgUL64K0n9JTlQVYM+\na/3kriQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5Ia\nY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEG\nvyQ1xuCXpMYY/JLUGINfkhpj8EtSY3oFf5JtSY4lmU2ya4H5G5KcSXKoe9wxb34iyTeS3DuqwiVJ\ny7NmsQVJJoA7gRuBk8D+JPuq6ui8pQ9V1c0XOc3twKPAr1xKsZKkS9fnin8LMFtVx6vqPLAH2N73\nCZJsAN4GfHx5JUqSRqlP8K8HTgztn+zG5tua5HCS+5NcPzT+EeD9wC+e7UmS7Egyk2Tm9OnTPcqS\nJC3HqF7cPQhMVdVrgY8CewGS3Aw8UVUHFjtBVe2uqkFVDSYnJ0dUliRpvj7BfwrYOLS/oRt7RlWd\nrapz3fZ9wNok64A3Am9P8l3mbhG9OcmnR1G4JGl5+gT/fmBzkk1JrgCmgX3DC5JckyTd9pbuvE9W\n1QeqakNVXdsd95WqetdIO5AkLcmi7+qpqgtJbgMeACaAu6vqSJKd3fxdwC3ArUkuAE8B01VVK1i3\nJGmZ8lzM58FgUDMzM6tdhiQ9byQ5UFWDPmv95K4kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEv\nSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLU\nGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmN6BX+SbUmOJZlN\nsmuB+RuSnElyqHvc0Y2/KMnXk3wzyZEkHxp1A5KkpVmz2IIkE8CdwI3ASWB/kn1VdXTe0oeq6uZ5\nYz8F3lxV55KsBf4tyf1V9R+jKF6StHR9rvi3ALNVdbyqzgN7gO19Tl5zznW7a7tHLatSSdJI9An+\n9cCJof2T3dh8W5McTnJ/kuufHkwykeQQ8ATwYFU9vNCTJNmRZCbJzOnTp5fQgiRpKUb14u5BYKqq\nXgt8FNj79ERV/byqXgdsALYkec1CJ6iq3VU1qKrB5OTkiMqSJM3XJ/hPARuH9jd0Y8+oqrNP39Kp\nqvuAtUnWzVvzY+CrwLZLqliSdEn6BP9+YHOSTUmuAKaBfcMLklyTJN32lu68TyaZTPLSbvxK5l4g\nfmyUDUiSlmbRd/VU1YUktwEPABPA3VV1JMnObv4u4Bbg1iQXgKeA6aqqJK8EPtW9M+gFwGer6t6V\nakaStLhUPffeZDMYDGpmZma1y5Ck540kB6pq0Getn9yVpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9J\njTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQY\ng1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMasWe0CRumdez/GqavWrXYZkvT/MrSZuZ28\nIKxdsxaAiYmruPKqVwHwmhdfyV9t3rDiJXnFL0mNGasr/nvecetqlyBJz3m9rviTbEtyLMlskl0L\nzN+Q5EySQ93jjm58Y5KvJjma5EiS20fdgCRpaRa94k8yAdwJ3AicBPYn2VdVR+ctfaiqbp43dgF4\nX1UdTPIS4ECSBxc4VpJ0mfS54t8CzFbV8ao6D+wBtvc5eVU9XlUHu+2fAI8C65dbrCTp0vUJ/vXA\niaH9kywc3luTHE5yf5Lr508muRZ4PfDwMuqUJI3IqF7cPQhMVdW5JDcBe4HNT08meTHweeC9VXV2\noRMk2QHsAJiamhpRWZKk+fpc8Z8CNg7tb+jGnlFVZ6vqXLd9H7A2yTqAJGuZC/3PVNUXLvYkVbW7\nqgZVNZicnFxiG5KkvvoE/35gc5JNSa4ApoF9wwuSXJPukwlJtnTnfbIb+wTwaFV9eLSlS5KWY9Fb\nPVV1IcltwAPABHB3VR1JsrObvwu4Bbg1yQXgKWC6qirJm4A/Br6V5FB3yr/ofiuQJK2CVNVq1/BL\nBoNBzczMrHYZkvS8keRAVQ36rPVPNkhSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5Ia\nY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEG\nvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNaZX8CfZluRYktkkuxaYvyHJ\nmSSHuscdQ3N3J3kiySOjLFyStDyLBn+SCeBO4K3AdcA7k1y3wNKHqup13eMvh8Y/CWwbRbGSpEvX\n54p/CzBbVcer6jywB9je9wmq6mvAj5ZZnyRpxPoE/3rgxND+yW5svq1JDie5P8n1I6lOkjRya0Z0\nnoPAVFWdS3ITsBfYvJQTJNkB7ACYmpoaUVmSpPn6XPGfAjYO7W/oxp5RVWer6ly3fR+wNsm6pRRS\nVburalBVg8nJyaUcKklagj7Bvx/YnGRTkiuAaWDf8IIk1yRJt72lO++Toy5WknTpFg3+qroA3AY8\nADwKfLaqjiTZmWRnt+wW4JEk3wT+FpiuqgJIcg/w78Crk5xM8p6VaESS1E+6fH5OGQwGNTMzs9pl\nSNLzRpIDVTXos9ZP7kpSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklq\njMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY\n/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNaZX8CfZluRYktkkuxaYvyHJmSSHuscdfY+V\nJF1eaxZbkGQCuBO4ETgJ7E+yr6qOzlv6UFXdvMxjJUmXSZ8r/i3AbFUdr6rzwB5ge8/zX8qxkqQV\n0Cf41wMnhvZPdmPzbU1yOMn9Sa5f4rGSpMtk0Vs9PR0EpqrqXJKbgL3A5qWcIMkOYAfA1NTUiMqS\nJM3X54r/FLBxaH9DN/aMqjpbVee67fuAtUnW9Tl26By7q2pQVYPJyckltCBJWoo+wb8f2JxkU5Ir\ngGlg3/CCJNckSbe9pTvvk32OlSRdXove6qmqC0luAx4AJoC7q+pIkp3d/F3ALcCtSS4ATwHTVVXA\ngseuUC+SpB4yl8/PLYPBoGZmZla7DEl63khyoKoGfdb6yV1JaozBL0mNMfglqTEGvyQ1xuCXpMYY\n/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEv\nSY0x+CWpMQa/JDXG4Jekxhj8ktSYVNVq1/BLkpwGvrfMw9cBPxxhOc8HLfYMbfbdYs/QZt9L7flV\nVTXZZ+FzMvgvRZKZqhqsdh2XU4s9Q5t9t9gztNn3SvbsrR5JaozBL0mNGcfg373aBayCFnuGNvtu\nsWdos+8V63ns7vFLkp7dOF7xS5KexdgEf5JtSY4lmU2ya7XrWSlJNib5apKjSY4kub0bf3mSB5N8\nu/v3Zatd66glmUjyjST3dvst9PzSJJ9L8liSR5P87rj3neTPuu/tR5Lck+RF49hzkruTPJHkkaGx\ni/aZ5ANdvh1L8keX8txjEfxJJoA7gbcC1wHvTHLd6la1Yi4A76uq64A3AH/S9boL+HJVbQa+3O2P\nm9uBR4f2W+j5b4AvVtVvAr/FXP9j23eS9cCfAoOqeg0wAUwznj1/Etg2b2zBPrv/49PA9d0xf9fl\n3rKMRfADW4DZqjpeVeeBPcD2Va5pRVTV41V1sNv+CXNBsJ65fj/VLfsU8I7VqXBlJNkAvA34+NDw\nuPf8q8DvAZ8AqKrzVfVjxrxvYA1wZZI1wFXAfzOGPVfV14AfzRu+WJ/bgT1V9dOq+g4wy1zuLcu4\nBP964MTQ/slubKwluRZ4PfAw8Iqqeryb+gHwilUqa6V8BHg/8IuhsXHveRNwGviH7hbXx5NczRj3\nXVWngL8Gvg88Dpypqn9ljHue52J9jjTjxiX4m5PkxcDngfdW1dnhuZp7q9bYvF0ryc3AE1V14GJr\nxq3nzhrgt4GPVdXrgf9l3i2Oceu7u6e9nbkfer8BXJ3kXcNrxq3ni1nJPscl+E8BG4f2N3RjYynJ\nWuZC/zNV9YVu+H+SvLKbfyXwxGrVtwLeCLw9yXeZu4335iSfZrx7hrmrupNV9XC3/znmfhCMc99/\nAHynqk5X1c+ALwBbGe+eh12sz5Fm3LgE/35gc5JNSa5g7kWQfatc04pIEubu+T5aVR8emtoHvLvb\nfjfwL5e7tpVSVR+oqg1VdS1zX9uvVNW7GOOeAarqB8CJJK/uht4CHGW8+/4+8IYkV3Xf629h7nWs\nce552MX63AdMJ3lhkk3AZuDry36WqhqLB3AT8J/AfwEfXO16VrDPNzH3699h4FD3uAn4NebeBfBt\n4EvAy1e71hXq/wbg3m577HsGXgfMdF/vvcDLxr1v4EPAY8AjwD8CLxzHnoF7mHsd42fM/Xb3nmfr\nE/hgl2/HgLdeynP7yV1Jasy43OqRJPVk8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1Jj/\nAzXDfA3fC9/aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f44fd41efd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np ## For numerical python\n",
    "#np.random.seed(42)\n",
    "\n",
    "class Layer:\n",
    "    \n",
    "    #A building block. Each layer is capable of performing two things:\n",
    "\n",
    "    #- Process input to get output:           output = layer.forward(input)\n",
    "    \n",
    "    #- Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    #Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Here we can initialize layer parameters (if any) and auxiliary stuff.\n",
    "        # A dummy layer does nothing\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
    "        \n",
    "        # A dummy layer just returns whatever it gets as input.\n",
    "        return input\n",
    "    \n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        # Performs a backpropagation step through the layer, with respect to the given input.\n",
    "        \n",
    "        # To compute loss gradients w.r.t input, we need to apply chain rule (backprop):\n",
    "        \n",
    "        # d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "        \n",
    "        # Luckily, we already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        # If our layer has parameters (e.g. dense layer), we also need to update them here using d loss / d layer\n",
    "        \n",
    "        # The gradient of a dummy layer is precisely grad_output, but we'll write it more explicitly\n",
    "        num_units = input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule\n",
    "    \n",
    "    \n",
    "    \n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        # ReLU layer simply applies elementwise rectified linear unit to all inputs\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Apply elementwise ReLU to [batch, input_units] matrix\n",
    "        relu_forward = np.maximum(0,input)\n",
    "        return relu_forward\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        # Compute gradient of loss w.r.t. ReLU input\n",
    "        relu_grad = input > 0\n",
    "        return grad_output*relu_grad\n",
    "    \n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        # A dense layer is a layer which performs a learned affine transformation:\n",
    "        # f(x) = <W*x> + b\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.normal(loc=0.0, \n",
    "                                        scale = np.sqrt(2/(input_units+output_units)), \n",
    "                                        size = (input_units,output_units))\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        # Perform an affine transformation:\n",
    "        # f(x) = <W*x> + b\n",
    "        \n",
    "        # input shape: [batch, input_units]\n",
    "        # output shape: [batch, output units]\n",
    "        return np.dot(input,self.weights) + self.biases \n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        # compute d f / d x = d f / d dense * d dense / d x\n",
    "        # where d dense/ d x = weights transposed\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        \n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        grad_weights = np.dot(input.T, grad_output)\n",
    "        grad_biases = grad_output.mean(axis=0)*input.shape[0]\n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        \n",
    "        # Here we perform a stochastic gradient descent step. \n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "    \n",
    "# def softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "#     m = reference_answers.shape[0]\n",
    "#     p = softmax(logits)\n",
    "#     x = range(m)\n",
    "#     print(reference_answers.shape)\n",
    "#     log_likelihood = -np.log(p)\n",
    "#     loss = np.sum(log_likelihood) / m\n",
    "#     return loss\n",
    "\n",
    "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    sigmoid = 1/(1+np.exp(logits))\n",
    "    list1 = []\n",
    "    for i in range(0,32):\n",
    "        if reference_answers[i]==1:\n",
    "            return -np.log(sigmoid)\n",
    "        if reference_answers[i]==0:\n",
    "            return -np.log(1-sigmoid)\n",
    "            \n",
    "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    sigmoid = 1/(1+np.exp(logits))\n",
    "    list2 = []\n",
    "    for i in range(0,32):\n",
    "        if reference_answers[i]==1:\n",
    "            return sigmoid-1\n",
    "        if reference_answers[i]==0:\n",
    "            return sigmoid\n",
    "    \n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import scipy\n",
    "\n",
    "def load_dataset(flatten=False):\n",
    "    #(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    X_train = genfromtxt('/home/karthik/Desktop/Epilepsy detection/set1x.csv',delimiter = ',')\n",
    "    y_train = genfromtxt('/home/karthik/Desktop/Epilepsy detection/sety1.csv',delimiter = ',')\n",
    "    X_test =  genfromtxt('/home/karthik/Desktop/Epilepsy detection/p1x.csv',delimiter = ',')\n",
    "    y_test =  genfromtxt('/home/karthik/Desktop/Epilepsy detection/p1y.csv',delimiter = ',')\n",
    "    \n",
    "    return X_train, y_train,X_test, y_test\n",
    "\n",
    "X_train, y_train,X_test, y_test = load_dataset(flatten=True)\n",
    "\n",
    "\n",
    "network = []\n",
    "\n",
    "network.append(Dense(12,100))\n",
    "network.append(ReLU())\n",
    "f = [100,200,300,400]\n",
    "for i in range(len(f)-1):\n",
    "    network.append(Dense(f[i],f[i+1]))\n",
    "    network.append(ReLU())\n",
    "network.append(Dense(f[-1],1))\n",
    "\n",
    "def forward(network, X):\n",
    "    # Compute activations of all network layers by applying them sequentially.\n",
    "    # Return a list of activations for each layer. \n",
    "    \n",
    "    activations = []\n",
    "    input = X\n",
    "\n",
    "    # Looping through each layer\n",
    "    for l in network:\n",
    "        activations.append(l.forward(input))\n",
    "        # Updating input to last layer output\n",
    "        input = activations[-1]\n",
    "    \n",
    "    assert len(activations) == len(network)\n",
    "    return activations\n",
    "\n",
    "def predict(network,X):\n",
    "    # Compute network predictions. Returning indices of largest Logit probability\n",
    "\n",
    "    logits = forward(network,X)[-1]\n",
    "    return logits.argmax(axis=-1)\n",
    "\n",
    "def train(network,X,y):\n",
    "    # Train our network on a given batch of X and y.\n",
    "    # We first need to run forward to get all layer activations.\n",
    "    # Then we can run layer.backward going from last to first layer.\n",
    "    # After we have called backward for all layers, all Dense layers have already made one gradient step.\n",
    "    \n",
    "    \n",
    "    # Get the layer activations\n",
    "    layer_activations = forward(network,X)\n",
    "    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n",
    "    logits = layer_activations[-1]\n",
    "    \n",
    "    # Compute the loss and the initial gradient\n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
    "    \n",
    "    # Propagate gradients through the network\n",
    "    # Reverse propogation as this is backprop\n",
    "    for layer_index in range(len(network))[::-1]:\n",
    "        layer = network[layer_index]\n",
    "        \n",
    "        loss_grad = layer.backward(layer_inputs[layer_index],loss_grad) #grad w.r.t. input, also weight updates\n",
    "        \n",
    "    return np.mean(loss)\n",
    "\n",
    "from tqdm import trange\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []\n",
    "for epoch in range(100):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=False):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    #val_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    #print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    #plt.plot(val_log,label='val accuracy')\n",
    "    #plt.legend(loc='best')\n",
    "    #plt.grid()\n",
    "    #plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# input array\n",
    "X = genfromtxt('/home/karthik/Desktop/Epilepsy detection/set1x.csv',delimiter=',')  # train data\n",
    "\n",
    "X[0][0] = 0\n",
    "\n",
    "# output array\n",
    "Y = genfromtxt('/home/karthik/Desktop/Epilepsy detection/sety1.csv',delimiter=',')  # train data\n",
    "Y = np.reshape(Y,(1,7343))\n",
    "\n",
    "Y[0][0] = -1\n",
    "\n",
    "test = genfromtxt('/home/karthik/Desktop/Epilepsy detection/p1x.csv',delimiter=',')\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(test.shape)\n",
    "\n",
    "\n",
    "X_flat = X.T\n",
    "W = np.random.rand(12, 1) * 0.01\n",
    "b = 0\n",
    "m = Y.size\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))\n",
    "\n",
    "# Activation Function\n",
    "\n",
    "def activation_function(z):\n",
    "    #s = np.tanh(z)\n",
    "    s = 1 / (1 + (np.exp(-z)))\n",
    "    #leak = 0.2\n",
    "    #f1 = 0.5 * (1 + leak)\n",
    "    #f2 = 0.5 * (1 - leak)\n",
    "    #s = f1 * z + f2 * abs(z)\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "epoch = 2000  # Setting training iterations\n",
    "lr = 0.7  # Setting learning rate\n",
    "print(\"Training.....\")\n",
    "for i in range(epoch):\n",
    "    # Forward Propogation\n",
    "    Z = np.dot(W.T, X_flat) + b\n",
    "    A = activation_function(Z)\n",
    "    #print(A)\n",
    "    cost = -(1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A), axis=1, keepdims=True)\n",
    "    # Backpropagation\n",
    "    dZ = Y - A\n",
    "    dw = 1 / m * np.dot(X_flat, (A - Y).T)\n",
    "    db = 1 / m * np.sum(A - Y)\n",
    "    W = W - lr * dw\n",
    "    b = b - lr * db\n",
    "\n",
    "vali = np.array(activation_function(np.dot(W.T, X_flat) + b))\n",
    "\n",
    "\n",
    "#print(vali)\n",
    "#print(vali.shape)\n",
    "\n",
    "print(vali.shape)\n",
    "\n",
    "vali = np.reshape(vali,(1,7343))\n",
    "\n",
    "a_vali = np.where(vali > 0,1,-1)\n",
    "\n",
    "Y_true = a_vali.flatten()\n",
    "Y_pred = Y.flatten()\n",
    "print(\"Accuracy is .........\")\n",
    "accuracy = (np.abs(Y_true - Y_pred) < 0 ).all(axis=(0)).mean()\n",
    "print(accuracy_score(Y_true,Y_pred) * 100)\n",
    "\n",
    "\n",
    "#accu = np.asarray([vali == y for y in Y])\n",
    "\n",
    "#accu = np.reshape(accu,(1,22559))\n",
    "#print(accu.shape)\n",
    "\n",
    "#print(np.sum(accu))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Y_new = np.reshape(Y_new,(1,891))\n",
    "#print(accuracy_score(Y_true,Y_new) * 100)\n",
    "\n",
    "M = test  # test data\n",
    "#M [0][0] = 0\n",
    "M_flat = M.T\n",
    "print(W, b)\n",
    "out = np.array(activation_function(np.dot(W.T, M_flat) + b))\n",
    "out_1 = np.where(out > 0,1,-1)\n",
    "out_1 = out_1.T\n",
    "np.savetxt(\"foo_4.csv\", out_1, delimiter=\",\")\n",
    "print(out_1)\n",
    "\n",
    "Y_new = genfromtxt('/home/karthik/Desktop/Epilepsy detection/p1y.csv',delimiter=',')  # testing with this\n",
    "Y_new = np.reshape(Y_new,(1,891))\n",
    "Y_new[0][0] = -1\n",
    "print(out.shape)\n",
    "print(Y_new.shape)\n",
    "#print(accuracy_score(out_1,Y_new) * 100)\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_true=Y_true,y_pred=Y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for d in np.nditer(out, op_flags=['readwrite']):\n",
    "#    print(d)\n",
    "\n",
    "#print(np.shape(out))\n",
    "#print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
